---
title: 'Data: exploration & transformation'
jupyter: python3
---

<!-- <font size='6'>E-commerce Analytics</font><br><br>
<font size='6'><b>Sales & Market Basket Analysis</b></font><br><br>

<font size='4'>Antonio Buzzelli</font><br>
<font size='4'>March 2023</font> -->


The e-commerce industry has experienced significant growth in recent years, and online sales have become an increasingly important aspect of many businesses. Analyzing sales data can help businesses understand customer behavior and identify trends, which can then be used to improve their overall sales strategies. In this notebook, we will be analyzing a sales dataset from an e-commerce company to gain insights into their sales patterns. Our analysis will cover various aspects of the data, including temporal trends and customer geographical segmentation. We will also be performing a market basket analysis to identify relationships between products and suggest strategies for improving sales. By the end of this notebook, we aim to provide a comprehensive understanding of the sales data, which can then be used to make informed decisions and drive business growth.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:33.230074Z', start_time: '2023-04-02T18:30:32.875279Z'}
# %pip install pandas statsmodels mlxtend jupyterthemes
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:33.449961Z', start_time: '2023-04-02T18:30:32.875508Z'}
#| _cell_guid: b1076dfc-b9ad-4769-8c92-a6c4dae69d19
#| _uuid: 8f2839f25d086af736a60e9eeb907d3b93b6e0e5
# dataframes
import numpy as np
import pandas as pd

# dataviz
import matplotlib.pyplot as plt
import seaborn as sns
# from jupyterthemes import jtplot
# jtplot.style(theme='monokai', context='notebook', grid=False)

# hypothesis testing
from scipy.stats import ttest_ind
from scipy.stats import f_oneway
from statsmodels.stats.multicomp import pairwise_tukeyhsd

# market basket analysis
from itertools import permutations
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
```



Before diving into any analysis, it is essential to explore and transform the data to ensure its quality, completeness, and correctness. In this chapter, we will be exploring and transforming the e-commerce sales dataset to prepare it for our analysis. Our primary goal will be to check for data integrity, handle missing values, and ensure that all variables are in the correct format. We will also be creating new variables that will be useful in our analysis. This chapter will provide a foundation for the rest of our analysis, and the data exploration and transformation techniques used here will be applicable in many other scenarios. By the end of this chapter, we will have a cleaned and prepared dataset ready for our analysis, allowing us to focus on extracting meaningful insights.


```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:35.128988Z', start_time: '2023-04-02T18:30:32.893242Z'}
df = pd.read_csv('sales.csv')
df.info()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:35.151807Z', start_time: '2023-04-02T18:30:32.893509Z'}
df
```

Here above, a glimpse of the data at our disposal. The dataset is composed by the following original variables:

* TransactionNo (categorical): a six-digit unique number that defines each transaction. The letter “C” in the code indicates a cancellation.
* Date (numeric): the date when each transaction was generated.
* ProductNo (categorical): a five or six-digit unique character used to identify a specific product.
* Product (categorical): product/item name.
* Price (numeric): the price of each product per unit in pound sterling (£).
* Quantity (numeric): the quantity of each product per transaction. Negative values related to cancelled transactions.
* CustomerNo (categorical): a five-digit unique number that defines each customer.
* Country (categorical): name of the country where the customer resides.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:35.958997Z', start_time: '2023-04-02T18:30:32.908924Z'}
# Validating variable types
df['Date'] = pd.to_datetime(df['Date'])
df[['ProductNo', 'CustomerNo']] = df[['ProductNo', 'CustomerNo']].astype('object')

# Splitting `Date` column
df['Month'] = df['Date'].dt.month
df['Weekday'] = df['Date'].dt.weekday
df['WeekdayName'] = df['Date'].dt.day_name()
df['Day'] = df['Date'].dt.day

# Creating a `TotalPrice` colum
df['Amount'] = df['Quantity'] * df['Price']
```

## Returns

The most notable characteristic of this dataset is the presence of *returns*, represented by negative values in the `Quantity` variable. In this section we are going to gain insights on this feature to understand how to deal with it for further analysis.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:36.039441Z', start_time: '2023-04-02T18:30:32.920891Z'}
# We create a boolean variable to identify if a product is returned or not
df['Return'] = np.where(df['Quantity'] < 0, 1, 0)

# Create a boolean variable to show if the product is within a broader order or not
df['SingleOrder'] = df.duplicated('TransactionNo').astype(int)
print(f"Percentage of products sold within a broader order: {1 - sum(df['SingleOrder']) / df.shape[0]}")

# Creating another `TotalPrice` column that takes into consideration returns (represented by -n values in the `Quantity` column).
df['AmountN'] = np.where(df['Amount'] < 0, 0, df['Amount'])
```

Do returned products (return-labelled observations) come in specific transactions or are they mixed with non-returned products? 

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:36.190622Z', start_time: '2023-04-02T18:30:32.933021Z'}
transactionReturned = df.loc[df['Return'] == 1, 'TransactionNo']
df[(df['Return'] == 0) & (df['TransactionNo'].isin(transactionReturned))]
```

Returned products have specific transaction IDs (`TransactionNo`).

Do all returned-labelled observations begin with the letter 'C'?

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:36.203476Z', start_time: '2023-04-02T18:30:32.946830Z'}
np.all(np.char.startswith(transactionReturned.unique().astype(str), 'C'))
```

Comparing the total number of observations in which `TransactionNo` starts with 'C' with the total number of observations that have been precedently labelled as "returns":

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:36.485790Z', start_time: '2023-04-02T18:30:32.959297Z'}
return2 = np.where(df['TransactionNo'].str.startswith('C'), 1, 0)
print(df['Return'].sum())
print(return2.sum())
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:36.724447Z', start_time: '2023-04-02T18:30:32.959505Z'}
transno_c = df[df['TransactionNo'].str.slice(0, 1) == 'C']['TransactionNo'].str.slice(1)
df[df['TransactionNo'].isin(transno_c)]
```

Transaction IDs for returned products are different from the IDs of non-returned products, and they begin with letter 'C'.

## Missing values

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:37.586739Z', start_time: '2023-04-02T18:30:32.983426Z'}
(df.isnull().sum() / df.shape[0]).sort_values(ascending=False)
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:37.634970Z', start_time: '2023-04-02T18:30:32.983587Z'}
#| scrolled: true
nulls = df[df['CustomerNo'].isnull()]
nulls.head()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:37.704741Z', start_time: '2023-04-02T18:30:32.983646Z'}
nulls['Return'].value_counts()
```

Only `CustomerNo` variable has null values. They are most probably due to mistakes in data collection.

# Sales analysis

We will now be analyzing online sales data from a one-year period spanning from 2018-12-01 to 2019-11-30. The first type of analysis will focus on the temporal aspect of the data. This analysis aims to understand the sales evolution over time, as well as identify trends within months and weeks. The second type of analysis will center around examining the regional spread of sales in order to evaluate the existing market segmentation and gain insights into potential opportunities.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:37.756699Z', start_time: '2023-04-02T18:30:33.007675Z'}
# Subsetting for one exact year
df = df[df['Date'] <= '2019-11-30']
```

## Monthly evolution

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:38.158235Z', start_time: '2023-04-02T18:30:33.020896Z'}
month_evo = df.groupby(pd.Grouper(key='Date', freq='M')).agg(
    sold=('Amount','sum'), returned=('Amount', lambda x: sum(x[x < 0])),
    nunique=('TransactionNo', 'nunique'))
month_evo['sold_moving_avg'] = month_evo['sold'].rolling(window=3).mean()
month_evo['returned'] = month_evo['returned'].abs()
month_evo.index = month_evo.index.date
month_evo
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:38.170762Z', start_time: '2023-04-02T18:30:33.021061Z'}
month_evo_sum = month_evo[['sold', 'returned']].sum(axis=1)
month_evo_pct = month_evo[['sold', 'returned']].div(month_evo_sum, axis=0)
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:38.754106Z', start_time: '2023-04-02T18:30:33.021129Z'}
fig, ax = plt.subplots(2, 1, figsize=(15,10))
month_evo[['sold', 'returned']].plot.bar(ax=ax[0])
ax[0].set_ylabel('Revenue (GBP)')
ax[0].set_xlabel('Month')
ax[0].set_title("Monthly evolution of sales and returns")
ax[0].grid(axis='y')

month_evo_pct.plot.bar(stacked=True, ax=ax[1])
ax[1].set_ylabel('Percentage')
ax[1].set_xlabel('Month')
ax[1].set_title("Monthly relative amounts of sold and returned")
ax[1].grid(axis='y')

plt.subplots_adjust(hspace=0.5)
plt.show()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:39.634876Z', start_time: '2023-04-02T18:30:33.021185Z'}
fig, ax1 = plt.subplots(figsize=(15,5))
ax2 = plt.twinx()
ax1.plot(month_evo.index, month_evo['sold'], label='Revenue')
ax1.plot(month_evo.index, month_evo['sold_moving_avg'], label='3-month revenue moving average')
ax2.bar(month_evo.index, month_evo['nunique'], width=8, label='Volume', alpha=0.25)

ax1.set_ylabel('Revenue (GBP)')
ax2.set_ylabel('Volume')
ax1.set_xlabel('Month')
plt.title("Monthly evolution of sales")
plt.grid(True)
ax1.legend(loc=(0.025,0.85))
ax2.legend(loc=(0.3,0.85))

plt.show()
```

## Intra-month analysis

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:39.833511Z', start_time: '2023-04-02T18:30:33.036535Z'}
df = df[df['Quantity'] > 0]

bydate = df.groupby('Date').agg(
    UniqueTransactions=('TransactionNo', 'nunique'),
    UniqueProdSold=('TransactionNo', 'count'),
    ProdSold=('Quantity', 'sum'),
    Revenue=('Amount', 'sum')
    ).reset_index()
bydate['Day'] = bydate['Date'].dt.day
bydate['Weekday'] = bydate['Date'].dt.weekday
bydate['Month'] = bydate['Date'].dt.month

bydate['WeekdayName'] = bydate['Date'].dt.day_name()
bydate
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:39.845687Z', start_time: '2023-04-02T18:30:33.036737Z'}
byday = bydate.groupby('Day')[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()
byday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']
byday = byday.sort_index()
byday.head()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:40.256555Z', start_time: '2023-04-02T18:30:33.036798Z'}
rev_coefficients = np.polyfit(byday.index.values, byday['DailyAvgRev'].values, 5)
rev_regression_line = np.poly1d(rev_coefficients)

fig, ax1 = plt.subplots(figsize=(15,5))
ax2 = plt.twinx()
ax2.plot(byday.index, byday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)
ax1.bar(byday.index, byday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)
ax2.plot(rev_regression_line(byday.index.values), label='Regression line')
ax2.axhline(byday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Monthly average')

ax1.set_ylabel('N. transactions')
ax2.set_ylabel('Revenue (GBP)')
plt.title("Intra-month sales analysis")
plt.grid(True)
ax1.legend(loc='upper left')
ax1.set_xlabel('Day')
ax2.legend()

plt.show()
```

By analyzing the revenue data within a month, we can observe that the daily average revenue varies throughout the month. The revenue reaches its peak at around three-quarters of the month and dips to its lowest point just before the end of the month. However, it starts to increase again just before the last few days. The dip in revenue just before the end of the month is considered normal as it coincides with the time when people typically receive their salaries.

## Intra-week analysis

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:40.272884Z', start_time: '2023-04-02T18:30:33.063239Z'}
byweekday = bydate.groupby(['Weekday', 'WeekdayName'])[['UniqueTransactions', 'UniqueProdSold', 'ProdSold', 'Revenue']].mean()
byweekday.columns = ['DailyAvgUniqueTransactions', 'DailyAvgUniqueProdSold', 'DailyAvgProdSold', 'DailyAvgRev']
byweekday = byweekday.reset_index().set_index('Weekday')
byweekday.index = byweekday.index + 1
byweekday
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:40.671720Z', start_time: '2023-04-02T18:30:33.063457Z'}
rev_coefficients = np.polyfit(byweekday.index.values, byweekday['DailyAvgRev'].values, 2)
rev_regression_line = np.poly1d(rev_coefficients)

fig, ax1 = plt.subplots(figsize=(15,5))
ax2 = plt.twinx()
ax2.plot(byweekday['WeekdayName'], byweekday['DailyAvgRev'], label='Daily average revenue', alpha=0.3)
ax1.bar(byweekday['WeekdayName'], byweekday['DailyAvgUniqueTransactions'], label='Daily average unique transactions', alpha=0.1)
ax2.plot(rev_regression_line(byweekday.index.values), label='Regression line')
ax2.axhline(byweekday['DailyAvgRev'].mean(), color='b', linestyle='dashed', linewidth=1, label='Weekly average')

ax1.set_ylabel('N. transactions')
ax2.set_ylabel('Revenue(GBP)')
plt.title("Intra-week sales analysis")
plt.grid(axis='y')
ax1.legend(loc='lower left')
ax1.set_xlabel('Weekday')
ax2.legend()

plt.show()
```

Similar to the analysis conducted within a month, examining sales patterns within a week can also reveal interesting insights. By looking at the graph above, it becomes evident that the sales volume and revenue significantly increase during the latter part of the week. Specifically, revenue exceeds the weekly average starting from Thursday. On the other hand, Wednesday remains the least profitable day of the week with the lowest sales volume and revenue.

## Geographical analysis

When conducting a geographical analysis of sales, it is essential to consider both the average purchase value and sales volume to determine if there are any countries that offer promising opportunities. For instance, a country with a high average purchase value but low sales volume may indicate that it has untapped potential and should be targeted for further penetration. The average purchase value gives an indication of the buying power and willingness of customers to spend money, while sales volume reflects the market demand and potential for growth. A country with a high average purchase value and low sales volume could be a potential opportunity for businesses to capitalize on the untapped market potential by increasing their presence and promoting their products or services more effectively.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:40.824045Z', start_time: '2023-04-02T18:30:33.088723Z'}
# Mapping regions
regions = {'Europe': ['Sweden', 'Denmark', 'Norway', 'Finland', 'Iceland', 'Netherlands', 'Belgium', 'France', 'Germany', 'Switzerland', 'Austria',
                      'Italy', 'Spain', 'Greece', 'Portugal', 'Malta', 'Cyprus', 'Czech Republic', 'Lithuania', 'Poland', 'United Kingdom', 'EIRE',
                      'Channel Islands', 'European Community'],
           'North America': ['USA', 'Canada'],
           'Middle East': ['Bahrain', 'United Arab Emirates', 'Israel', 'Lebanon', 'Saudi Arabia'],
           'Asia Pacific': ['Japan', 'Australia', 'Singapore', 'Hong Kong'],
           'RoW': ['Brazil', 'RSA'],
           'Unspecified': ['Unspecified']}

country_to_region = {}
for region, countries in regions.items():
    for country in countries:
        country_to_region[country] = region

df['Region'] = df['Country'].map(country_to_region)

df['UKvsRoW'] = np.where(df['Country'] == 'United Kingdom', 'UK', 'RoW')
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:40.888654Z', start_time: '2023-04-02T18:30:33.089011Z'}
bycountry = df.groupby('Country').agg(
    tot_amount=('Amount', 'sum'),
    mean_amount=('Amount', 'mean')
).sort_values('tot_amount', ascending=False)
bycountry.head()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.036360Z', start_time: '2023-04-02T18:30:33.089149Z'}
fig, ax = plt.subplots(2, figsize=(15,10))
ax[0].bar(bycountry.index, bycountry['tot_amount'])
ax[1].bar(bycountry.sort_values('mean_amount', ascending=False).index, bycountry.sort_values('mean_amount', ascending=False)['mean_amount'])
plt.setp(ax, xticks=bycountry.index, xticklabels=bycountry.index)
plt.setp(ax[0].get_xticklabels(), rotation=90, ha="center")
plt.setp(ax[1].get_xticklabels(), rotation=90, ha="center")

ax[0].set_ylabel("Amount (GBP)")
ax[1].set_ylabel("Amount (GBP)")
ax[0].set_title("Countries by total amount sold")
ax[1].set_title("Countries by average amount sold")
plt.suptitle("Overview on geographical market spread")
ax[0].grid(axis='y')
ax[1].grid(axis='y')
plt.subplots_adjust(hspace=0.7)

plt.show()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.180659Z', start_time: '2023-04-02T18:30:33.089242Z'}
byukvsrow = df.groupby('UKvsRoW').agg(
    tot_amount=('Amount', 'sum'),
    mean_amount=('Amount', 'mean'),
    n_inv=('TransactionNo', 'nunique'),
    quantity=('Quantity', 'mean')
).sort_values('mean_amount', ascending=False)
byukvsrow
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.341127Z', start_time: '2023-04-02T18:30:33.089334Z'}
plt.pie(byukvsrow['tot_amount'], labels=byukvsrow.index, autopct='%1.1f%%', explode=(0.1,0), shadow=True)
plt.title('Total revenue by UK vs other countries')
plt.show()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.475006Z', start_time: '2023-04-02T18:30:33.089420Z'}
row_rev = df.loc[df['UKvsRoW'] == 'RoW', 'Amount']
uk_rev = df.loc[df['UKvsRoW'] == 'UK', 'Amount']

ttest_ind(uk_rev, row_rev)
```

> Even though the volume of sales of international customers accounts only for the 17.0%, the **average revenue generated abroad is significantly higher than the one generated in the UK**. This means that international markets for this business are potentially more lucrative than the national one and need to be exploited more.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.613739Z', start_time: '2023-04-02T18:30:33.106391Z'}
byregion = df.groupby('Region').agg(
    tot_amount=('Amount', 'sum'),
    mean_amount=('Amount', 'mean'),
    n_inv=('TransactionNo', 'nunique'),
    quantity=('Quantity', 'mean')
).sort_values('mean_amount', ascending=False)
byregion.sort_values('mean_amount', ascending=False)
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:42.834612Z', start_time: '2023-04-02T18:30:33.106571Z'}
fig, ax1 = plt.subplots(figsize=(15,5))
ax1 = plt.bar(byregion.index, byregion['mean_amount'])
plt.title("Average purchase value by region")
plt.ylabel('Amount (GBP)')
plt.xlabel('Region')
plt.grid(axis='y')
plt.show()
```

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:30:44.456895Z', start_time: '2023-04-02T18:30:33.106638Z'}
f_value, p_value = f_oneway(df.loc[df['Region'] == 'Asia Pacific', 'Amount'],
                            df.loc[df['Region'] == 'North America', 'Amount'],
                            df.loc[df['Region'] == 'Middle East', 'Amount'],
                            df.loc[df['Region'] == 'Europe', 'Amount'],
                            df.loc[df['Region'] == 'RoW', 'Amount'])
print(f'ANOVA F-value: {f_value:.2f}')
print(f'ANOVA p-value: {p_value:.4f}')
tukey_df = df.filter(items=['Amount', 'Region']).dropna()
print(pairwise_tukeyhsd(tukey_df['Amount'], tukey_df['Region']))
```

> We can observe from both the bar plot and the ANOVA analysis that the **mean purchase value in the Asia/Pacific region is consistently and significantly higher** than the mean purchase value in the other regions. Based on this important information, we can infer that the Asia/Pacific region is a potentially lucrative market with higher average purchase amounts than the other regions. Therefore, the store may want to consider investing more resources in this region to take advantage of this opportunity to increase volume of sales. The business can consider implementing targeted marketing strategies, such as advertising campaigns and promotions, that cater to the preferences and interests of the Asia/Pacific market. Additionally, it can explore expanding its product offerings to meet the specific demands of this region, or enhancing the quality of existing products to meet their higher standards. It may be useful to conduct further research and analysis to gain deeper insights into the preferences and behavior of customers in the Asia/Pacific region, and tailor sales strategies accordingly.

# Market basket analysis for the Asian market

Market basket analysis is a powerful technique in the field of data mining and business analytics that has gained significant attention in recent years. It enables businesses to understand the relationships between products and customer behavior, helping them make informed decisions about their product offerings, promotions, and pricing strategies. The technique is widely used in industries such as retail, e-commerce, and marketing, where understanding customer purchasing patterns is crucial for improving sales and profitability.

In market basket analysis, association rules play a central role as they describe the relationships between sets of items using if-then statements. For instance, an association rule like {razor} {shaving cream} can be interpreted as "if razor, then shaving cream," where razor is the antecedent and shaving cream is the consequent. It is worth noting that many rules have multiple antecedents and consequents. To begin with, the first step of a market basket analysis involves extracting a unique dataframe of products and then elaborating on the association rules.

```{python}
#| ExecuteTime: {end_time: '2023-04-02T18:31:00.989793Z', start_time: '2023-04-02T18:30:33.170680Z'}
#| tags: []
# Converting transactions in a list of lists
transactions = df.groupby('TransactionNo').apply(lambda x: list(x['ProductName'])).to_list()
encoder = TransactionEncoder().fit(transactions)
onehot = encoder.transform(transactions)
onehot = pd.DataFrame(onehot, columns=encoder.columns_)
```

```{python}
frequent_itemsets = apriori(onehot,
                            min_support = 0.02, 
                            max_len = 5, 
                            use_colnames = True)
print('Number of itemsets selected:', len(frequent_itemsets))
```

```{python}
#| scrolled: true
#| tags: []
rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.51)
rules['n_antecedents'] = rules['antecedents'].apply(lambda x: len(x))
rules['n_consequents'] = rules['consequents'].apply(lambda x: len(x))
rules.sort_values('confidence', ascending=False)
```

These MBA metrics are commonly used in association rule mining, a data mining technique used to identify relationships and patterns among items in a dataset. Here's a brief explanation of each metric:

* Antecedent support: This refers to the proportion of transactions that contain the antecedent (or the "if" part of a rule). It is calculated as the number of transactions containing the antecedent divided by the total number of transactions.

* Consequent support: This refers to the proportion of transactions that contain the consequent (or the "then" part of a rule). It is calculated as the number of transactions containing the consequent divided by the total number of transactions.

* Support: This refers to the proportion of transactions that contain both the antecedent and the consequent. It is calculated as the number of transactions containing both the antecedent and the consequent divided by the total number of transactions.

* Confidence: This measures the strength of the association between the antecedent and the consequent. It is calculated as the support of the antecedent and consequent divided by the support of the antecedent. Confidence can range from 0 to 1, with higher values indicating stronger associations.

* Lift: This measures the degree to which the presence of the antecedent affects the likelihood of the consequent. It is calculated as the support of the antecedent and consequent divided by the product of the support of the antecedent and the support of the consequent. A lift value greater than 1 indicates a positive association between the antecedent and consequent, while a value less than 1 indicates a negative association.

* Leverage: This measures the difference between the observed frequency of the antecedent and consequent co-occurring and the frequency expected if they were independent. It is calculated as the support of the antecedent and consequent minus the product of the support of the antecedent and the support of the consequent. A positive leverage value indicates a positive association between the antecedent and consequent, while a negative value indicates a negative association.

* Conviction: This measures the degree of implication of the rule. It is calculated as the ratio of the support of the antecedent to the complement of the confidence. Conviction can range from 0 to infinity, with higher values indicating stronger implications.

```{python}
support_table = rules.pivot(index='consequents', columns='antecedents', values='support')
sns.heatmap(support_table)
```

```{python}
#| tags: []
sns.scatterplot(rules, x='support', y='confidence', size='lift')
```

